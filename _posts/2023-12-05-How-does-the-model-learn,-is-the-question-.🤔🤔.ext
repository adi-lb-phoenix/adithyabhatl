This article will give you a small intuition on how the model learn .

step1 : predict the values

step 2 : calculate the loss

step 3 : go backward on the loss . i’m not kidding .

step 4 : calculate the gradient

step 5 : optimize the parameter

What matters the most for prediction of the values is the parameter values .

```python
import torch
import numpy
import matplotlib.pyplot as plt

time=torch.arange(0,20).float() # we have the variable time which is of 20s 
speed=torch.randn(20) + 0.75*(time - 9.5)**2 + 1
#so "speed" above is what we want to predict for a range of 20s as indicated by time

def f(time,params):
          t=time
          a,b,c=params 
         return a*(t**2) + b*(t) + c
#this is the function used to predict the value of speed for every second t

def mse(pred,targets):
        return ((targets - pred)**2).mean()
#this here is our loss function . loss function depends on prediction which depends on f()

params = torch.randn(3).requires_grad_()
orig_params=params.clone()```

in “params = torch.randn(3).requires_grad_()” if the “requires_grad_()” isnt added , 
then the pytorch will not be able to find the derivate of something w.r.t params . Hence this line of code is massively important .

```python 
def show_preds(preds,ax=None):
        if ax is None:
        ax=plt.subplots()[1]
ax.scatter(time,speed)
ax.scatter(time,preds.detach().numpy(),color='red')
ax.set_ylim(-300,200)
ax.set_xlim(0,20)
ax.plot()
plt.show()
```

